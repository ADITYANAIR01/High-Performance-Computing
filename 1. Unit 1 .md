    Introduction to Parallel and Distributed Computing

Parallel computing is  the concurrent use of multiple computer resources to solve a computational problem

High performance computing is the use of high-speed processors, clusters, and networks to perform complex calculations and data analysis.

Data parallelism is the execution of the same task on different data sets by multiple processing cores.

Flynn's taxonomy is a classification of computer architectures based on the number and type of instruction and data streams.

Array processor is a processor that performs operations on large arrays of data using a single instruction stream and multiple data streams (SIMD).

Multiprocessor is a system with two or more CPUs that share a common memory and can execute multiple tasks concurrently (MIMD).

Multicomputer is a system with multiple processors that have their own private memory and communicate via a network (MIMD).

Message passing interface (MPI) is a standard for inter-process communication in parallel and distributed computing.


    Advantages and Disadvatages of Parallel computing

* Advantages 

• Because multiple resources work together to reduce time and expenses, it saves time and money. 

• You can use a lot of computing resources at once to do several tasks. For modeling, simulating, and interpreting complex real-world events.

 
* Disadvantages

• Power usage for multi-core designs is high. Due to the complexity of communication and coordination,parallel solutions are more challenging to implement,
debug, and prove correct, and they frequently perform worse than their serial counterparts

    
    Distributing Computing

It consists of a number of software parts that are installed on many computers yet work together as a single system. 
The computers that make up a distributed system can be geographically separated and connected by a wide area network
or physically close to one another and connected by a local network (WAN). Mainframes, PCs, workstations, and
minicomputers are just a few examples of the many distinct configurations that might make up a distributed system. 
Making a network function as a single computer is the primary goal of distributed computing.

* Advantages

Because of its adaptability, installing, utilizing, and debugging new services is a breeze.If more machines are needed
you can add them in distributed computing. Other servers are unaffected if the system crashes on one server.
A distributed computer system can speed up traditional systems by combining the computational power of numerous machines.

*  Disadvantages

Due to the characteristics of open systems, data security and sharing are the key problems in distributed systems. 
The dispersion over numerous servers makes troubleshooting and debugging more difficult. The absence of software support is
distributed computer systems' fundamental drawback

    Introduction to high performance computing

• High-speed data processing and intricate calculations are capabilities of high-performance computing (HPC). 

• High performance computing (HPC) enables the quick resolution of challenging computing issues.

• High-performance computers (shared-memory mainframes, computing clusters) comprised of numerous
parallel processors are used for computation.lt is used for many different things, from big data analysis and
machine learning to solving engineering challenges in the MATLAB environment. The supercomputer is
one of the most well-known forms of HPC solutions. Thousands of compute nodes are found in a
supercomputer, and they collaborate to finish one or more tasks. We refer to this as parallel processing. It
is comparable to having thousands of PCs networked together to combine computational power and speed up task completion.